{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1 style=\"font-size: 3em;\"> COGS 118A - Project Proposal </h1> </center>\n",
    "<br>\n",
    "<center> <h1> DOTA2 Hero-Selection Winrate Prediction </h1> </center>\n",
    "<br>\n",
    "\n",
    "<h1> Names </h1>\n",
    "<ul >\n",
    "  <li style=\"padding-left: 20px;\"> Jamie Wei (A15921963) </li>\n",
    "  <li style=\"padding-left: 20px;\"> Nicole Rangan (A15011222) </li>\n",
    "  <li style=\"padding-left: 20px;\"> Sarita Raghunath (A16425750) </li>\n",
    "  <li style=\"padding-left: 20px;\"> Zehong Li (A15852954) </li>\n",
    "</ul>\n",
    "\n",
    "<center> <h3> Link to our <a href=\"https://github.com/COGS118A/Group035-Wi23\">GitHub repository</a> </h3> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Abstract</h1>\n",
    "<p>The aim of this project is to predict the win-rate of Dota2 games based on the heroes picked, using data from OpenDota API. We used logistic regression, support vector machine (SVM), random forest, and neural network to predict the probability of a team winning, based on the combination of heroes picked by both teams. We will use Python libraries like NumPy, Pandas, Matplotlib, Seaborn, Tensorflow, Keras, and scikit-learn for data cleaning, handling, and model construction. <br><br>\n",
    "We collected data from the top 0.05% of players, consisting of the combinations of heroes used in gameplay, which will be one-hot encoded. Our data consists of 100,400  rows of observations, with 14 features included each. Our topic focuses only on the hero-selection of the two teams, as a result we will only keep three features (team_radiant pick, team_dire pick, win/loss label). The heroes picked will then be encoded into one feature of 10 inputs. The data then got cleaned and we ended up with 91,588 total observations for the input dataset. <br><br>\n",
    "While a valuable application of the research is to assist in drafting strategies for Dota2 games, we will frame our study as an exploration of the potential of machine learning algorithms in analyzing and predicting the outcomes of complex, team-based games. <br><br>\n",
    "We performed a train-test split with scikit-learn’s train_test_split tool, with a training size of 80% and test size of 20% and using the random_seed set at 42. For every model, we performed k-fold cross-validation for our grid search or random search to find the best parameters, with the k-value depending on the complexity of our algorithm. <br><br>\n",
    "Based on our research, we selected Wang and Shang’s Naive Bayes Classifier as our benchmark model that we will be comparing our models’ performance with. <a href=\"#ref6\" id=\"#ref1text\">[6]</a> We explored multiple algorithms, and evaluated their performance primarily with accuracy, though we are also considering other metrics like precision, recall, and ROC-AUC curve. After thorough experimentation and analysis, we have identified the algorithm that gives the highest metric performance score to be the ensembled voting classifier. The algorithm is then used to be compared with our benchmark model. <br><br>\n",
    "We found that the test accuracy of models across different algorithms varies around .55 (±0.03). The benchmark model has the highest accuracy of ~.59, and our models fail to perform better than the benchmark model. However, with an accuracy slightly better than random guessing (~.50), we concluded that heroes selection as a feature does not give us an effective and accurate prediction on the winrate of a Dota2 match. While certain limitations do affect the result of our model, we also concluded that many more factors that are related to the game can greatly improve the accuracy of the winrate. \n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Background </h1>\n",
    "<p>Defense of the Ancient 2 (DotA2) is a popular Multiplayer Online Battle Arena (MOBA) game played by two teams, each consisting of five players, with a pool of 123 heroes to choose from. The game is highly strategic, with an ultimate goal of destroying of opponents’ ‘ancient’, a heavily guarded structure by towers, creeps, and opponents’ players (McDonald, 2013) <sup><a href=\"#ref1\" id=\"#ref1text\">[1]</a></sup> . The game requires teams to coordinate effectively, and make crucial decisions during drafting and gameplay, as the combination of heroes picked can significantly affect the outcome of the game (Walbridge 2008) <sup><a href=\"#ref2\" id=\"#ref1text\">[2]</a></sup>. With the competitive characteristic and its popularity, DotA2 has one of the largest esport tournament in the world, with the highest prize pool to record (Wang et al. 2015) <sup><a href=\"#ref3\" id=\"#ref1text\">[3]</a></sup>.<br><br>\n",
    "Given its esport competitive nature and commercial value on tournaments, various scholars and researchers conducted model construction to estimate or predict the winning probability (win-rate) of a game. DotA2’s developer company Valve have also published their own game win-rate forecast model, included in their subscription service Dota Plus (2018) <sup><a href=\"#ref4\" id=\"#ref1text\">[4]</a></sup>. American artificial intelligence research laboratory OpenAI has also stepped into the area of DotA2, but not with a win-rate prediction model, but rather a feature that was embedded in their DotA2 AI \"OpenAI Five\"<sup><a href=\"#ref5\" id=\"#ref1text\">[5]</a></sup> . However, their winrate prediction is also based on in-game situation, and the model itself is never a classifier but rather a deep reinforcement learning model. <br><br>\n",
    "However, according to Wang and Shang in their paper in 2017<sup><a href=\"#ref6\" id=\"#ref1text\">[6]</a></sup>, ‘no clear algorithms or software’ are specifically designed for the task of predicting outcome ‘by analyzing the lineups’, in other words, the heroes composition (p. 1). Most algorithm exists are based on in-game stats, such as kill/death/assist ratio and gold per minutes (GPM), but hardly any algorithms are designed only for analyzing matchups, the heroes picked or banned. Wang and Shang created Naïve Bayes classifier for a prediction model, but there is a significant difference in their training accuracy and test accuracy. <br><br>\n",
    "A good classification model in DotA2, or more generally in esport area, can be important for the games’ competitive scene. As esport growing into professional sport environments today, such a model can have practical applications in the gaming industry, particularly in assisting players with drafting strategies to improve their chances of winning. Additionally, this study can inspire further research into developing models for predicting the outcomes of other complex, team-based games, demonstrating the broad applicability of machine learning algorithms to a variety of domains.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Problem Statement </h1>\n",
    "<p>The problem addressed in our study is predicting the win-rate of Dota2 games based solely on the heroes picked by each team. The goal is to develop a model that can accurately predict the probability of a team winning the game, regardless of players' skills or patch. The problem is well-defined, as the dataset will consist of the 10 heroes picked by both teams, which will be one-hot encoded and used as input for the model.\n",
    "<br><br>\n",
    "The solution to the problem is through machine learning algorithms, which are capable of learning the past patterns on high win-rate hero combinations in training and utilize such observed patterns to predict the outcome based on hero composition.\n",
    "<br><br>\n",
    "The problem is quantifiable, as it can be expressed mathematically using the probability of winning as a metric. It is measurable, as the accuracy of the model's predictions can be observed and quantified. Finally, the problem is replicable, as it can be reproduced using different datasets and has occurred multiple times in Dota2 games.\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data </h1>\n",
    "<p style=\"padding-left: 20px;\">  <a href=\"https://docs.opendota.com/#\"> Open DotA - API <sup><a href=\"#ref7\" id=\"#ref1text\">[7]</a></sup> </a> <br><br>\n",
    "\n",
    "<ul>\n",
    "    <li> <h2> Data Collection </h2> </li>\n",
    "    <p>The data used in this study will be obtained from the OpenDota API, which provides access to game data such as match history, player profiles, and hero statistics. Each observation will consist of the combination of heroes picked by each team, as well as the win/loss outcome from the match history. The critical variables in the dataset will be the hero combination and the win rate. The data will be one-hot encoded to represent hero names and team combinations as binary variables. Our initial data collected from the API request consist of 100,400 observations and 14 features included in each observation, as shown in Fig 1.1 & Fig 1.2.</p> <br><br>\n",
    "\n",
    "   <img src=\"./img/fig1-1.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\"> Fig 1.1: Initial Observations of our data from the API request </p> <br>\n",
    "   \n",
    "   <img src=\"./img/fig1-2.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\">Fig 1.2: API request script that gathers matches data from OpenDOTA API </p> <br>\n",
    "    \n",
    "   <p>In our problem set, we only focus on how specific heroes picked by the radiant team and the dire team can affect the win/loss status of the match. As a result, we cleaned the data, with the consideration that certain other features might have an effect on our results. Here is the initially cleaned dataset, shown in Fig 1.3.</p> <br><br>\n",
    "    \n",
    "   <img src=\"./img/fig1-3.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\">Fig 1.3: initially cleaned data </p> <br>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "  <li> <h2> Data Cleaning </h2> </li>\n",
    "    <p>Our one-hot encode function is shown in Fig 1.4. It creates a new dataframe with the heroes index as the columns. Due to the fact that each observation consists of two teams, we created twice the amount of heroes index for both teams(team one: hero1_x, team two: hero2_x). Then we loop through each datapoint and check the condition and then encode into the correct index.</p> <br><br>\n",
    "    \n",
    "   <img src=\"./img/fig1-4.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\"> Fig 1.4: one hot encode function </p> <br>\n",
    "   \n",
    "   <img src=\"./img/fig1-5.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\">Fig 1.5: pre-one hot encode  </p> <br>\n",
    "   \n",
    "   <p> As shown in Fig 1.5, our data frame combines the radiant team and dire team into one team, so we can apply the one hot encode function. <br>\n",
    "    This code in Fig 1.6 runs the function from the column team in dataframe which is the combined teams. It splits the data points into 10 and encodes the first 5 in the first hero columns and the last 5 in the second hero columns.</p> <br><br>\n",
    "    \n",
    "   <img src=\"./img/fig1-6.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\"> Fig 1.6 Running function </p> <br>\n",
    "    \n",
    "   <p>This figure shows the dataframe we will be conducting our model on. Our end result contains 91588 rows and 250 columns. 246 columns are the one hot encoded column for each hero on two separate teams. </p> <br><br>\n",
    "   \n",
    "   <img src=\"./img/fig1-7.png\" style=\"display: block; margin: 0 auto;\">\n",
    "   <p style=\"text-align: center;\"> Fig 1.7 : Final Dataframe </p> <br>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "<h2> Steps </h2>\n",
    "<ol> \n",
    "  <li>Obtained data from the API and combined it into one super dataset.</li>\n",
    "  <li>Dropped all rows with NaN values, duplicate rows, and unnecessary columns.</li>\n",
    "  <li>Created a custom one-hot encoding function.</li>\n",
    "  <li>Added a new column in the dataset that combined the two teams. </li>\n",
    "  <li>Applied the custom one-hot encoding function to the column containing the combined teams. </li>\n",
    "  <li>Dropped the team columns (radiant_team, dire_team, and combined team) from the dataset.</li>\n",
    "</ol>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Proposed Solution</h1>\n",
    "<p> \n",
    "Our proposed solution to our problem set is to compare and contrast five different machine learning algorithm and see if the one with the best performance metric score does a better job at classifying than our base-model, the Naive Bayes Classifier from Wang and Shang (2017). <br><br>\n",
    "The five machine learning algorithm we will be testing on is: Logistic Regression, Support Vector Machine, Random Forest, Neural Network, and an ensemble voting classfier with the those models as base estimator. <br><br>\n",
    "We split the training data and testing data in a 80-20 portion, meaning 80% of data are used for training and 20% are used for testing.<br><br>\n",
    "We used k-fold cross validation with k = 5 for model validating purpose. By splitting our training data into 5 equal portions and training the model on 4 of the portions and use the remaining portion for validation. The process is repeated for 5 times and each time a different portion is used for validation. The final validation score is calculated through taking average of the 5 validation score. This will tell us what we should expect the accuracy of our model to be for new and unseen data. <br><br>\n",
    "As for hyperparameter tuning, we used grid-search with cv to find the optimal hyperparameters for our models. By trying different values for the regularization strength and the number of estimators in the random forest, we then evaluated each combinatioon of hyperparameters using 5-fold cv. We then choose the combination of hyperparameters that gave us the highest cv score to be our final model.<br><br>\n",
    "We will then compare our best model with Wang and Shang's Naive Bayes Classifier Model (2017) and draw conclusion based on the result. \n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Evaluation Metric</h1>\n",
    "<p> \n",
    "We plan to evaluate the performance of our model using a range of metrics, including accuracy, precision, recall, and the area under the receiver operating characteristic (ROC) curve (ROC-AUC). Accuracy will be used as the primary evaluation metric, which represents the proportion of correct predictions made by the model. In addition to accuracy, precision  and recall will be used to provide more detailed insight into the performance of the model. Precision measures the proportion of true positives out of all positive predictions, and recall measures the proportion of true positives out of all actual positives. Other metrics that we will use in case are f1-score, specificity, The ROC-AUC curve will be used to evaluate and visualize the trade-off between true positive rate and false positive rate at different probability thresholds. We will also use the PR-AUC curve that evaluates the positive predictive value against the true positive rate. Along with the visualizations provided through ROC-AUC and PR-AUC we will be using a confusion matrix to visualize each specific type of error (Figure 2.1). <br><br>\n",
    "Additionally, we used the sci-kit learn balanced accuracy report as a metric to evaluate class imbalances in the dataset. The function computes the balanced accuracy score as an average recall (true positive rate) and specificity (true negative rate) for each class (Figure 2.2). When using this function, we found that we have slightly imbalanced classes - there are more radiant wins compared to losses. <br><br>\n",
    "While we will still be using accuracy as our primary performance evaluation metric, it can be misleading to use with asymmetric or imbalanced data sets. In preparation for this, we will also be using additional metrics like precision and recall (as mentioned above), as well as f1-score (Figure 2.3), specificity (Figure 2.4), negative predictive value (Figure 2.5), false negative (Figure 2.6)  and false positive rates (Figure 2.7). All of these metrics will be used to compare performance between classifiers to identify the best suited one for our data. <br><br>\n",
    "    \n",
    "<img src=\"./img/fig2-1.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Figure 2.1: Formulas to calculate Precision, Recall, and Accuracy <sup><a href=\"#ref12\" id=\"#ref12text\">[6]</a></sup> </p> <br>\n",
    "\n",
    "<img src=\"./img/fig2-2&2-3.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Figure 2.2&2.3: Formula for balanced accuracy & Formula for f1-score <sup><a href=\"#ref12\" id=\"#ref12text\">[6]</a></sup></p> <br>\n",
    "\n",
    "<img src=\"./img/fig2-4&2-5.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Figure 2.4&2.5:  Formula for Specificity & Formula for Negative predictive value <sup><a href=\"#ref12\" id=\"#ref12text\">[6]</a></sup></p> <br>\n",
    "\n",
    "<img src=\"./img/fig2-6&2-7.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Figure 2.6&2.7: Formula for false negative rate & Formula for false positive rate <sup><a href=\"#ref12\" id=\"#ref12text\">[6]</a></sup></p> <br>\n",
    "\n",
    "However, there is no single metric that can capture the full performance of a machine learning model, and different metrics can be more or less appropriate depending on the specifics of the problem being solved. While we will use multiple evaluation metrics to gain a comprehensive understanding of our model's performance, we acknowledge that there are limitations to each metric and that some metrics may be more relevant or informative than others for our specific problem. <br>\n",
    "\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Results </h1>\n",
    "\n",
    "\n",
    "\n",
    "<h2> Preliminary Results & EDA </h2>\n",
    "<p> \n",
    "<img src=\"./img/fig3-1.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.1 Wins and Losses per Team </p> <br>\n",
    "\n",
    "Fig 3.1 shows the amount of wins we have for the Radiant team vs Dire team. There is a 30 game difference out of 90000. That's a 0.06% difference in win percentage. <br><br>\n",
    "\n",
    "<img src=\"./img/fig3-2.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.2: Distribution of win rates in the heroes </p> <br>\n",
    "\n",
    "As in Fig 3.2, we can see that the overall win-rate of heroes in Dota2 is in the range of ~.498 - ~.518, with an interquartile range of ~0.1 (from 0,495 to 0.505). We have an outlier that has a win-rate of 0.475. \n",
    "<p>\n",
    "\n",
    "    \n",
    "    \n",
    "<h2> Subsection 1: Why Our Models? </h2>\n",
    "<p> \n",
    "When selecting our proposed solutions, we had a large pool of algorithms to choose from. However, given the nature of our problem, there are only certain algorithms that would fit the goal of our problem. <br><br>\n",
    "Our model is a binary classifier, with a binary output of 1 or 0 (meaning win or loss of the radiant team). As a result, an algorithm with continuous output variables would be unsuitable for our problem. We first excluded the linear regression model. <br><br>\n",
    "At the same time, with 246 total inputs, algorithms like decision trees / stumps would not be ideal since our problem is complicated with a large number of input features. These models would tend to overfit on the training data and will not be able to provide correct predictions on new data given. <br><br>\n",
    "Another concern is that our feature space is very large and as a result, algorithms like k-Nearest Neighbours (k-NN) would not be able to produce a good result. In the case of our problem, the distance metric can be much less reliable, and either result in significant underfit or learns too much about the noise of our problem, hence overfit on our data. <br><br>\n",
    "With the goal of having algorithms that can handle large inputs but also gives a binary output or a probability, which can be normalized into either 1 or 0. The first ideal model is logistic regression model. Though simple and basic, it is fast and is able to handle a large number of input features. With proper hyperparameters, it can give us relatively good results, which can be later used to improve through ensemble learning methods.<br><br>\n",
    "We also selected neural networks, support vector machines, and random forest for their algorithmic complexity as well as their abilities to capture the complex relationships between features. With 246 input data, we have a high-dimensional vector space and such non-linear relationships can be well predicted and learned by the SVM.<br><br>\n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<h2> Subsection 2: Further Transformation on Our Data </h2>\n",
    "<p> \n",
    "As we described in the data section, we transformed our data using hot-encoding, with heroes picked indicated as 1 in their corresponding column, and the rest encoded as 0. As we got to the model construction part, we realized that not all models can predict a boolean (‘True’ / ‘False’) that we had in our data cleaning process. <br><br>\n",
    "As a result, we also changed our label into 0 and 1s, with 0 representing False, and 1 representing True. The code shown in Fig 3.3 indicated such process:\n",
    " \n",
    "<img src=\"./img/fig3-3.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.3 : Label Transformation </p> <br>\n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<h2> Subsection 3: The Benchmark Model </h2>\n",
    "<p> \n",
    "Our benchmark model comes from Wang and Shang’s paper in 2017, in which they used a Naive Bayes Classifier that takes the heroes as input and predicts on the probability of winning. According to Fig 3.4, they have a training accuracy of 0.8533 and a test accuracy of 0.5899m which indicates an overfitting problem. \n",
    " \n",
    "<img src=\"./img/fig3-4.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.4 : Naive Bayes Classifier Accuracy (Wang. & Shang. 2017) </p> <br>\n",
    "\n",
    "We seek to overcome the overfitting issue by performing grid search before training our model, seeking for the right hyperparameter that avoids overfitting, such as learning rate and penalty for our logistic regression model, dropout and regularization for neural network, and max-features and max-depth for a random forest. <br><br>\n",
    "We are also looking to achieve a higher test accuracy with our models. A 0.5899 accuracy is slightly better than random guessing. Our method suggested more complicated models that can capture the complexity among feature relationships. With more features (more heroes released since 2017) and more matches of data collected, we believe that a better dataset can also lead to a higher accuracy. \n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "<h2> Subsection 4: Model Selection and Hyperparameter Tuning </h2>\n",
    "<p> \n",
    "We first evaluated four different models for predicting the win rate in our dataset: SVM, Logistic Regression, Neural Network, and Random Forest. For each model, we conducted hyperparameter running to identify the best set of hyperparameters for each model, as seen in Table 3.1. <br><br>\n",
    "To optimize the hyperparameters of our models, we used Grid Search and Randomized Search. Grid Search is a technique that exhaustively searches over a specified hyperparameter space, while Randomized Search randomly samples a specified hyper parameter space. For SVM and Random Forest models, we used Randomized Search with a 2- fold cross-validation. For Logistic Regression and Neural Network models, we used Grid Search with a 3-fold and 2-fold cross-validation respectively due to the model complexity and time-cost. This allowed us to efficiently search a large hyperparameter space and find the best possible set of hyperparameters for each model. The table figure below shows the hyperparameters searched, and the best hyperparameters found from the search conducted. \n",
    "    \n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>HyperParameters Searched</th>\n",
    "      <th>Best Hyperparameters</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Logistic Regression: 2-fold cross-validation</td>\n",
    "      <td>{ 'solver': ['lbfgs', 'sag', 'newton-cg'],<br>\n",
    "        'penalty': ['none', 'l2'],<br>\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },<br><br>\n",
    "    {\n",
    "        'solver': ['liblinear'],<br>\n",
    "        'penalty': ['l1', 'l2'],<br>\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "    },<br><br>\n",
    "    {\n",
    "        'solver': ['saga'],<br>\n",
    "        'penalty': ['none', 'l1', 'l2', 'elasticnet'],<br>\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],<br>\n",
    "        'l1_ratio': [0.25, 0.5, 0.75]\n",
    "    }<br>\n",
    "</td>\n",
    "      <td>‘C’:10<br><br>\n",
    "‘l1_ratio’: 0.5<br><br>\n",
    "‘penalty’: ‘elasticnet’<br><br>\n",
    "‘solver’: ‘saga’<br><br>\n",
    "</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>SVM</td>\n",
    "      <td>    'C': [0.1, 1, 10],<br><br>\n",
    "    'kernel': ['linear', 'poly', 'rbf'],<br><br>\n",
    "    'gamma': ['scale', 'auto']<br><br>\n",
    "</td>\n",
    "      <td>‘C’: 0.1<br><br>\n",
    "‘gamma’: ‘scale’<br><br>\n",
    "‘kernel’: ‘linear’<br><br>\n",
    "       </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Random Forest</td>\n",
    "      <td>  'n_estimators': [100, 500, 1000],<br><br>\n",
    "    'criterion': [\"gini\", \"entropy\"],<br><br>\n",
    "    'max_depth': [3, 5, 7],<br><br>\n",
    "    \"min_samples_split\": [2, 5, 10],<br><br>\n",
    "    \"min_samples_leaf\": [1, 2, 4],<br><br>\n",
    "    'class_weight': ['balanced', 'balanced_subsample'],<br><br>\n",
    "    \"bootstrap\": [True, False]<br><br>\n",
    "        </td>\n",
    "      <td>  'N_estimators': 1000<br><br>\n",
    " \"Min_samples_split\": 5<br><br>\n",
    " \"min_samples_leaf\": ‘1<br><br>\n",
    "  'max_depth': 7<br><br>\n",
    "  criterion': ‘gini’<br><br>\n",
    " 'class_weight': ‘balanced’<br><br>\n",
    " \"bootstrap\": ‘True’<br><br>\n",
    "</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Neural Network</td>\n",
    "      <td>   'learning_rate': [0.001, 0.01],<br><br>\n",
    "    'hidden_layers': [1, 2, 3],<br><br>\n",
    "    'nodes': [32, 64],<br><br>\n",
    "    'activation': ['relu', 'sigmoid', 'tanh'],<br><br>\n",
    "    'regularization_strength': [0.01, 0.1]<br><br>\n",
    "</td>\n",
    "      <td>'activation': 'tanh', 'hidden_layers': 1, 'learning_rate': 0.001<br><br>\n",
    "'nodes':32<br><br>\n",
    "'regularization_strength': 0.01<br><br>\n",
    "</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"> Table 3.1 : Hyperparameter Search Space for each model </p> <br>\n",
    "\n",
    "After obtaining the best hyperparameters from the Grid Search and Randomized search for these models, we trained the model with these hyperparameters on the training set. <br><br>\n",
    "To obtain a more reliable estimate of the performance of our logistic regression and SVM models, we performed K-fold cross-validation. For logistic regression, we used a 5-fold cross-validation with shuffling of the data to ensure representative folds. For SVM, we used the best hyperparameters found during Randomized search and performed 5-fold cross validation on a randomly sampled subset of 100,000 training samples. We trained Logistic Regression and SVM models on each fold and calculated the accuracy on both training and validation sets. The learning curve results are shown in Fig 3.5 for logistic regression model and Fig 3.6 for support vector machine model:\n",
    "\n",
    "\n",
    "<img src=\"./img/fig3-5.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.5: Training and testing Accuracy of LR model with HyperParameter Tuning using 5-Fold Cross validation </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-6.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.6: Training and testing Accuracy of SVM model with Hyperparameter Tuning using 5-Fold Cross- Validation </p> <br>\n",
    "\n",
    "For the neural network model, we used Tensorflow Keras API backend to train and evaluate the model. We used the best hyperparameters obtained from the grid search to evaluate the model, and then trained it for 100 epochs with a batch size of 32. We also evaluated the model on the test set to obtain the test accuracy. The training history of the model was visualized using two plots- one for the training loss and the other for the training accuracy as shown in Fig 3.7. The test accuracy was found to be 0.5509, and test loss to be 0.6870.\n",
    "\n",
    "<img src=\"./img/fig3-7.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.7: Training loss and accuracy graph- Neural Network </p> <br>\n",
    "\n",
    "We then created a ‘show_metrics’ function to evaluate and compare the performance of each model. We used this function to obtain the training set accuracy, classification report on test set performance, confusion matrix, ROC and precision curves, balanced accuracy, specificity, negative predictive value, false positive rate, and false negative rate. We applied this function to our logistic regression, SVM, random forest, and neural network models. We also used the predict method to obtain both predicted train and test labels and calculated ‘yhat_te’ and ‘yhat_tr’ for each model. ‘yhat_te’ and ‘yhat_tr’ contain the predicted binary labels for the test and training sets, respectively, obtained from the best model. These labels are obtained by applying a threshold (in this case, 0.5) to the predicted probabilities of the positive class obtained by the model. This allowed us to compare the performance of our models on the test set using a variety of metrics. <br><br>\n",
    "Fig 3.8 showed our code for getting the predicted and test labels from our random forest model:\n",
    "\n",
    "<img src=\"./img/fig3-8.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.8: generating predicted labels </p> <br>\n",
    "\n",
    "Fig 3.9 showed our code for applying the ‘show_metric’  function to the RF model: \n",
    "\n",
    "<img src=\"./img/fig3-9.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.9: generating predicted labels </p> <br>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>Training Set Accuracy</th>\n",
    "      <th>Test Set Accuracy</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Neural Network</td>\n",
    "      <td>0.561</td>\n",
    "      <td>0.511</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Random Forest</td>\n",
    "      <td>0.588</td>\n",
    "      <td>0.548</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>SVM</td>\n",
    "      <td>0.562</td>\n",
    "      <td>0.548</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Logistic Regression</td>\n",
    "      <td>0.566</td>\n",
    "      <td>0.555</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"> Table 3.2: Training set accuracy, test set accuracy table for each  model </p> <br>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>Precision</th>\n",
    "      <th>Recall </th>\n",
    "      <th>F1- score </th>\n",
    "      <th>Accuracy</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Neural Network</td>\n",
    "      <td>0.5515</td>\n",
    "      <td>0.5512</td>\n",
    "      <td>0.5505</td>\n",
    "      <td>0.551</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Random Forest</td>\n",
    "      <td>0.5480</td>\n",
    "      <td>0.5479</td>\n",
    "      <td>0.5479</td>\n",
    "      <td>0.548</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>SVM</td>\n",
    "      <td>0.5483</td>\n",
    "      <td>0.5482</td>\n",
    "      <td>0.5482</td>\n",
    "      <td>0.548</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Logistic Regression</td>\n",
    "      <td>0.5505</td>\n",
    "      <td>0.5505</td>\n",
    "      <td>0.5505</td>\n",
    "      <td>0.550</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"> Table 3.3: Classification report on test set performance for each model </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-10.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.10 : Confusion Matrix of Logistic Regression (left) and SVM (right) </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-11&3-12.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.11 & 3.12 : Confusion Matrix of Neural Network (left) and Random Forest (right)</p> <br>\n",
    "\n",
    "In addition to calculating accuracy, precision, recall and F-1 score as metrics, we also used ROC (receiver operating characteristics) and precision- recall curves to evaluate the performance of each model.\n",
    "\n",
    "<img src=\"./img/fig3-13&3-14.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.13 & 3.14 : ROC and precision curve for Neural Network Model </p> <br>\n",
    "    \n",
    "<img src=\"./img/fig3-15&3-16.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.15 & 3.16 : ROC and precision curve for Random Forest  Model </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-17&3-18.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.17 & 3.18 : ROC and precision curve for SVM Model </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-19&3-20.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.19 & 3.20 : ROC and precision curve for Linear Regression Model </p> <br>\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Model</th>\n",
    "      <th>Balanced Accuracy</th>\n",
    "      <th>Neg. Pred. Value </th>\n",
    "      <th>False Pos.  Rate </th>\n",
    "      <th>False Neg. Rate</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Neural Network</td>\n",
    "      <td>0.551</td>\n",
    "      <td>0.543</td>\n",
    "      <td>0.414</td>\n",
    "      <td>0.483</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Random Forest</td>\n",
    "      <td>0.548</td>\n",
    "      <td>0.543</td>\n",
    "      <td>0.457</td>\n",
    "      <td>0.447</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>SVM</td>\n",
    "      <td>0.548</td>\n",
    "      <td>0.545</td>\n",
    "      <td>0.467</td>\n",
    "      <td>0.436</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Logistic Regression</td>\n",
    "      <td>0.550</td>\n",
    "      <td>0.545</td>\n",
    "      <td>0.445</td>\n",
    "      <td>0.454</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"> Table 3.4: Model Performance Metrics </p> <br>\n",
    "\n",
    "Based on our results metrics and evaluations provided, the neural network model performed the best among the four models. The neural network model had the highest precision, recall, F1- score and accuracy, all above 0.55, indicating a relatively higher overall performance in classifying target variables in comparison to the other models.Neural network had the highest balanced accuracy at 0.551 , and the lowest false positive rate at 0.414, indicating that it performed better in these areas compared to other models. However, it had a higher false negative rate compared to the logistic regression and random forest models. In terms of precision, recall, F1-score and accuracy, the neural network had relatively high scores. However, it is important to note that the overall performance of all models is not very high. The highest balanced accuracy achieved by the models is only 0.551, which indicates that the models are only slightly better than random guessing. \n",
    "\n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "<h2> Subsection 5: Model Prediction Results and Further Methods </h2>\n",
    "<p> \n",
    "\n",
    "<img src=\"./img/fig3-21.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.21 : Voting Classifier Structure </p> <br>\n",
    "\n",
    "The previous section indicated that most of our classifiers gave results that were low on accuracy, and have poor performance score on other metrics. Specifically, our best classifier neural network has an accuracy that is barely better than random guessing, close to our target of >.58. <br><br>\n",
    "We decided that using ensemble learning method with a majority voting can greatly improve the result accuracy, with the structure indicated in Fig 3.21. As shown in Fig 3.22, the ensemble error can be calculated as such:\n",
    "\n",
    "\n",
    "<img src=\"./img/fig3-22.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.22: Ensemble Error Calculation </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-23.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.23: Base Error vs. Ensemble Error projection </p> <br>\n",
    "\n",
    "In our case, our n = 3, and if k = 2, i.e. 2 classifiers predicted the same class label, with each classifier having an average error of .45, the ensemble error will equal to <math display=\"inline\">3 &times; .452 &times; .551 = .334</math>, giving us an accuracy of .66 or 66%. <br><br>\n",
    "As a result, we decides to use ensemble learning with hard voting to get a better result with our existing models. We first defined the three classifiers with their best parameters that was found by grid searching previously, as shown in Fig 3.24:  \n",
    "\n",
    "<img src=\"./img/fig3-24.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.24: Base Estimators </p> <br>\n",
    "\n",
    "We then construct the voting classifier using Sci-kit Learn’s VotingClassifier function, as shown in Fig 3.25:\n",
    "\n",
    "<img src=\"./img/fig3-25.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.25: Voting Classifier </p> <br>\n",
    "\n",
    "Since the parameters of the base estimators are already the best hyperparameters, and due to hardware limitation, we didn’t perform grid search on the voting classifier. The results of our voting_clf is displayed in Table 3.4, and from Fig 3.26  to Fig 3.28:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th> </th>\n",
    "      <th>Precision</th>\n",
    "      <th>Recall </th>\n",
    "      <th>F1-score </th>\n",
    "      <th>Accuracy</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Prediction</td>\n",
    "      <td>0.5550</td>\n",
    "      <td>0.6067</td>\n",
    "      <td>0.5797</td>\n",
    "      <td>0.5552</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "<p style=\"text-align: center;\"> Table 3.4: Classification Report on Ensemble Learning </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-26.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.26: Confusion Matrix of Ensemble Learning </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-27.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.27: ROC-AUC curve of Ensemble Learning </p> <br>\n",
    "\n",
    "<img src=\"./img/fig3-28.png\" style=\"display: block; margin: 0 auto;\">\n",
    "<p style=\"text-align: center;\"> Fig 3.28: PR Curve of Ensemble Learning </p> <br>\n",
    "\n",
    "From our results, as it can be seen from the confusion matrix, we have a better number of true-positive predictions, as well as a slight improve in precision-recall area under curve (+.01 from the random forest result), balanced accuracy (+.003 improvement from the neural network result), and negative predictive value (+.012 from both SVM and logistic regression result). However, with the test accuracy remains at ~.55, and the ROC-AUC at ~.55, the ensemble method did not improve our results much. We even reached a lower specificity than our previously least-specificity model (-.033 from SVM). \n",
    "\n",
    "<p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Discussion </h1>\n",
    "\n",
    "\n",
    "\n",
    "<h2> Interpreting the result </h2>\n",
    "<p> \n",
    "The main point to take away from our results is that  our models, including the ensemble voting model, fails to accurately capture the relationship between our input features and the winrate or win/loss label of Dota2 games. Our best model, the neural network, only showed slight improvement over random guessing (Table 3.3), and even with the ensemble method, our test accuracy remained around 55% (Table 3.4). These results indicate that the relationship between the input features and the winrate or win/loss label in Dota2 is intricate and cannot be readily captured by the models used in our study.<br><br>\n",
    "In comparison to the benchmark model, our models had a significantly lower training accuracy (Table 3.2), indicating that the Naive Bayes Classification used in the benchmark model may be a more suitable choice for predicting the outcome of Dota2 games. However, like previously stated, their model has high training accuracy and a relatively very low test accuracy, indicating a possibility of overfitting. Such overfitting can likely be avoided with proper methods and hyperparameter tuning. <br><br>\n",
    "Another point to consider is the potential limitations of using hero selection alone to predict game outcomes. This is seen in our results as our models had low accuracy and other performance scores, indicating that predicting win rates based on hero selection alone is not the most optimal method. Although our results show that there is a slight possibility to predict for winrates with hero selection, it does not give an accurate forecast on the result of the game. Theoretically an ensemble method should improve the accuracy and largely decrease the loss, but our actual result denied such hypothesis. Possible explanation might be that either our data is not sufficient enough for the models to learn the complex relationship between the features and the labels, or that the prediction of win rate using heroes alone is not very effective and will not result in an accurate prediction. <br><br>\n",
    "Overall, our results emphasize the complexity of predicting game outcomes in Dota2 and the limitations of using machine learning models with hero selection as the sole input feature. \n",
    "\n",
    "<p>\n",
    "\n",
    "   \n",
    "    \n",
    "<h2> Limitations </h2>\n",
    "<p> \n",
    "Certain limitations, including technical, time, and knowledge, might play a role in our low accuracy result. To begin with, our data set may not be sufficient or may not be good enough for the models to be trained on. Our machine learning models were trained on a sample of around 90,000 Dota 2 games, which compared to the total population is a relatively small number(approximately 0.09%). When machine learning models only work with a very small percentage of the data, there may be a chance of overfitting and suffer from biases. In the future, some data augmentations and other methods may improve the accuracy of this model. Moreover, with specific insight into the model such as specific features or combinations of features that are accessible and interpretable may give us better insight on how certain feature combinations can affect the winrate of a game, while the others decrease it. <br><br>\n",
    "Secondly, the features that we selected as our input might not be enough to give an accurate prediction on win rate. While empirically, the heroes picked by a team can be a significant factor in a game, a team’s player skillset, random events in game, patch metas (certain heroes that have significantly advantage over others), team coordination, game strategy, average rank score / mmr (an overall measurement on the quality of a match) and many other factors can lead to a different result. In our case, heroes' selection alone may not be able to capture the complexity in win or loss of a match. <br><br>\n",
    "Also, our models may not be well-trained due to hardware limitations. Our best approach was with Tensorflow that utilized GPU computation. We do not have the state-of-the-art GPUs, and not to mention the little to none computational optimization in Sci-kit learn. The support vector machine random search took us about 6 to 7 hours to find the best hyperparameters, as a grid search that gives us a more accurate hyperparameter tuning can take us even longer. We also limited the search space of our grid search to decrease the amount of permutation so that it would not take a significant amount of time. <br><br>\n",
    "Which leads to our last point: time constraints. We would be able to explore more possible hyperparameters for all of our models if we are allowed to run a model for a much longer time. But such time is hugely limited by UCSD class schedules, as well as hardware issues that we mentioned earlier. \n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<h2> Ethics & Privacy </h2>\n",
    "<p> \n",
    "While the use of the Dota2 API to gather player data is based on user consent, there remains a risk of the model being used to create unfair advantages for certain players or teams. Analyzing player data to identify their weaknesses could create an unfair playing field and undermine the integrity of the game. Therefore, it is crucial to use the model's results ethically and we will ensure that the data is anonymized to protect the privacy of the players. We collected data with anonymity of players and our data includes nothing that may cause concern of players' confidential information. Additionally, it is important to be transparent about the purpose and methodology of the model to prevent misunderstandings and promote ethical usage. This research is conducted purely for academic purposes and should not be used unethically or against its intended purpose. Overall, our study follows ethical guidelines and respect the players' privacy and rights, while still advancing the understanding of machine learning applications in complex team-based games.\n",
    "<p>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "<h2> Conclusion </h2>\n",
    "<p> \n",
    "With a test accuracy of roughly around 0.55 (± 0.03) across different machine learning algorithms, we conclude that heroes selection as a feature is not enough and cannot draw an accurate prediction on the win-rate of a Dota2 match. Previous studies have shown that while hero selection is an important factor in determining the outcome of a match, it is not the only factor, and other features such as player skill, team coordination, and game strategy can also play a significant role.<br><br>\n",
    "In terms of future work, one possible direction is to incorporate additional features into the model to improve its accuracy. For example, player performance metrics such as kills, deaths, and assists (KDA), or players’ rank score (also known as mmr) could be added as features. Another direction is to explore more advanced machine learning algorithms or hybrid models that combine multiple approaches, such as deep learning and reinforcement learning.<br><br>\n",
    "Another potential avenue of research is to investigate the effectiveness of the model in different game modes or at different skill levels. It is possible that hero selection may be a more significant factor in certain modes or at certain levels of play.<br><br>\n",
    "Finally, it may be valuable to investigate the interpretability of the model and gain insight into the specific features or combinations of features that are most predictive of the win-rate. This could help to inform game strategy and provide a better understanding of the factors that contribute to success in Dota2.\n",
    "<p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Footnotes </h2>\n",
    "<ol>\n",
    "  <li id='ref1' style=\"padding-left: 20px; font-size: 10px\">  McDonald, P. (2013). A beginner's guide to Dota 2: Part one - The basics. PC Invasion. Retrieved from https://www.pcinvasion.com/a-beginners-guide-to-dota-2-part-one-the-basics/\n",
    " </li>\n",
    "  <li id='ref2' style=\"padding-left: 20px; font-size: 10px\"> Walbridge, D. (2008). Gamasutra - The art & business of making games. Gamasutra. Retrieved from https://web.archive.org/web/20121013125257/http://www.gamasutra.com/php-bin/news_index.php?story=18863#.UTN871fohYQ\n",
    " </li>\n",
    "  <li id='ref3' style=\"padding-left: 20px; font-size: 10px\"> Wang, H., Xia, B., & Chen, Z. (2015). Cultural difference on team performance between Chinese and Americans in multiplayer online battle arena games. In G. Rau, Y. Kurosu, & J. Cao (Eds.), Cross-Cultural Design Applications in Mobile Interaction, Education, Health, Transport and Cultural Heritage: 7th International Conference, CCD 2015, Held as Part of HCI International 2015, Los Angeles, CA, USA, August 2-7, 2015, Proceedings, Part II (pp. 374-383). Springer International Publishing.\n",
    " </li>\n",
    "  <li id='ref4' style=\"padding-left: 20px; font-size: 10px\"> Valve Corporation. (2018). Product release - Dota Plus. Steam News. Retrieved from https://store.steampowered.com/oldnews/38154\n",
    " </li>\n",
    "  <li id='ref5' style=\"padding-left: 20px; font-size: 10px\"> OpenAI. (n.d.). OpenAI Five. Retrieved February 22, 2023, from https://openai.com/five/\n",
    " </li>\n",
    "  <li id='ref6' style=\"padding-left: 20px; font-size: 10px\"> Wang, K., & Shang, W. (2017). Outcome prediction of DOTA2 based on Naïve Bayes classifier. In 2017 IEEE/ACIS 16th International Conference on Computer and Information Science (ICIS) (pp. 591-593). IEEE.\n",
    " </li>\n",
    "  <li id='ref7' style=\"padding-left: 20px; font-size: 10px\"> OpenDota. (n.d.). OpenDota API documentation. Retrieved February 22, 2023, from https://docs.opendota.com/#\n",
    " </li>\n",
    "  <li id='ref8' style=\"padding-left: 20px; font-size: 10px\"> Huang, J. (2015). Predicting the winning team of Dota 2. Retrieved from http://jmcauley.ucsd.edu/cse258/projects/fa15/018.pdf\n",
    " </li>\n",
    "  <li id='ref9' style=\"padding-left: 20px; font-size: 10px\"> Kaushik, S. & Kalyanaraman, K. (2015). Dota 2 match outcome prediction. Retrieved from https://cseweb.ucsd.edu/classes/wi15/cse255-a/reports/wi15/Kaushik_Kalyanaraman.pdf\n",
    " </li>\n",
    "  <li id='ref10' style=\"padding-left: 20px; font-size: 10px\"> Kaggle. (n.d.). Dota 2: Win prediction. Retrieved from https://www.kaggle.com/c/dota-2-win-probability-prediction\n",
    " </li>\n",
    "  <li id='ref11' style=\"padding-left: 20px; font-size: 10px\"> Vu, P. (2018). Dota 2 win-rate prediction. Retrieved from https://github.com/vpus/dota2-win-rate-prediction-v1\n",
    " </li>\n",
    "  <li id='ref12' style=\"padding-left: 20px; font-size: 10px\"> Ghasemi, J., Kabir, M., & Yuan, Y. (2019). Analyzing the Leading Causes of Traffic Fatalities Using XGBoost and Grid-Based Analysis: A City Management Perspective. Retrieved from ResearchGate website:https://www.researchgate.net/publication/336402347_Analyzing_the_Leading_Causes_of_Traffic_Fatalities_Using_XGBoost_and_Grid-Based_Analysis_A_City_Management_Perspective\n",
    " </li>\n",
    "</ol>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
